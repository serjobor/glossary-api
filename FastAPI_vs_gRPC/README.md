# 5. Сравнение производительности приложений на FastAPI. REST и gRPC

## Задание
Сравнить с помощью Locust приложения с глоссарием на предмет производительности их работы

Освоить методы проведения нагрузочного тестирования сетевых сервисов (REST и gRPC), научиться использовать инструмент Locust для генерации нагрузки, анализа поведения приложения под разными профилями нагрузки и формализации результатов.

1. **Развернуть приложения, разработанные в рамках предыдущих заданий (словарь терминов):**
FastAPI с использованием REST-подхода. 
приложение-словарь с использованием подхода RPC и передачи данных по протоколу protobuf. 

2. **Приложение должно содержать минимум 2 разных эндпоинта/метода, отличающихся логикой или трудоёмкостью.**
**Настроить нагрузочное тестирование с помощью Locust:**
Написать один или два класса пользователей (User/GrpcUser) в зависимости от тестируемых протоколов.
Смоделировать реалистичное поведение клиентов: разные сценарии запросов, пропорции между ними, паузы, последовательности действий.
Настроить параметры запуска Locust: количества пользователей, скорость подъёма нагрузки, длительность теста.

3. **Провести тестирование по нескольким сценариям нагрузки:**
- Лёгкая нагрузка (sanity check): убедиться, что всё работает.
- Рабочая нагрузка (нормальный режим): подобрать параметры, имитирующие реалистичное использование.
- Стресс-тест (приближение к пику): выявить пределы производительности.
- Тест на стабильность (при длительной нагрузке): проверить деградацию.
- Зафиксировать метрики:

- RPS (запросов в секунду),
- среднее время ответа,
- распределение латентности (p95/p99),
- количество ошибок,
- момент наступления деградации,
- влияние увеличения числа пользователей.
- Сравнить результаты REST и gRPC:

- обнаружить различия в пропускной способности и задержках,
- оценить влияние размера сообщений, сериализации, network-overhead.
- Требования к отчёту
- Отчет разместить в репозитории, отразите отчет с помощью файла с разметкой Markdown, где демонстрировался бы процесс развертывания и работы сервиса.

Опишите тестируемое приложения (архитектура, используемые технологии, использовалась ли БД, какие данные возвращаются, какие данные необходимы для выполнения запроса на добавление).

1. **Укажите настройки тестовой среды**
- аппаратные ресурсы (CPU, RAM, сеть),
- архитектуру стенда (что где запущено),
- версию Locust,
- дополнительные инструменты мониторинга (если использовались).

2. **Тестовые сценарии**
**Для каждого сценария описать:**
- логика поведения пользователя (task flow),
- конфигурация нагрузки (пользователи, spawn rate, длительность),
- ожидания перед запуском (гипотезы).
- Приложить фрагменты тестового кода Locust.

3. **Результаты тестирования**

**Для каждого сценария включить:**

3.1. **Основные метрики:** 
- RPS / Throughput,
- среднее время ответа,
- p95/p99 latency,
- количество ошибок (5xx/timeout/connection errors),
- графики или таблицы (можно экспорт Locust или собственные визуализации).

3.2. **Анализ результатов:** 
- На каком количестве пользователей начинается деградация?
- Как изменяется латентность при росте нагрузки?
- Где «бутылочное горлышко» — CPU, база данных, сеть, сама реализация сервиса?
- Отличаются ли результаты REST и gRPC?

4. **Сравнить REST и gRPC**
- численное сравнение латентности,
- сравнение RPS,
- анализ overhead,
- выводы о применимости каждого подхода.

**В заключении обязательно включить:**
- основные выводы,
- рекомендации по оптимизации,
- возможные улучшения эксперимента,
- ограничения проведённого тестирования.

**Исследовательская задача**
- Составьте подборку статей, где проводилось бы сравнение реализаций микросервисной архитектуры с помощью подходов REST и RPC, 
- GraphQL. 

Приоритет — исследования, где выполнялись бы замеры и бенчмарки. Приведите наиболее важные результаты таких исследований. Составьте краткое резюме этих исследований. 